"""

Main executable for the document structure parser. 

Usage:

   % python main.py [-h] [-c COLLECTION] [-l LANGUAGE] TEXT_FILE FACT_FILE STRUCTURE_FILE
   % python main.py [-h] [-c COLLECTION] [-l LANGUAGE] XML_FILE STRUCTURE_FILE
   % python main.py [-c COLLECTION] [-l LANGUAGE] FILE_LIST
   % python main.py [-c COLLECTION] [-l LANGUAGE] DIRECTORY
   % python main.py -o XML_FILE
   % python main.py -t

In the first form, input is taken from TEXT_FILE, which contains the bare text, and
FACT_FILE, which contains some structural tags taken from the low-level input parser. The
output is written to STRUCTURE_FILE, which has lines like the following

   SECTION ID=1 TYPE="UNLABELED" START=0 END=3978
   SECTION ID=2 TYPE="INTRODUCTION" TITLE="INTRODUCTION" START=3978 END=6016

If the -h option is specified, html versions of the fact file and the sect file will be
created and saved as FACT_FILE.html and SECT_FILE.html.

In the third form, the input and output files are specified in the file FILE_LIST. In the
fourth form, all pairs of .txt and .fact files in DIRECTORY are processed and .sect files
are created.

The optional [-h COLLECTION] argument specifies the collection that the input document was
taken from. This can be used to overrule the default behaviour, which is to scan the fact
file and find the following line:

   DOCUMENT COLLECTION="$COLLECTION"

In this line, $COLLECTION is in ('WEB_OF_SCIENCE', 'LEXISNEXIS', 'PUBMED', 'ELSEVIER').

Simliarly, with [-l LANGUAGE} the language can be handed in as an argument. Values are
'ENGLISH', 'GERMAN' and 'CHINESE'. As with the collection, the default behaviour is to
scan the fact file if there is one, searching for

   DOCUMENT LANGUAGE="EN|CH|DE"

In the fifth form, an XML file is taken and an input file for ontology creation is
generated. This is currentyl only relevant for patents. One step here is the
utils/create_standoff.pl script, which splits the XML file in text, meta and tag files.

Finally, in the sixth form, a simple sanity check is run, where four files (one pubmed,
one mockup Elsevier, one mockup WOS and one patent) are processed and the diffs between
the resulting .sect files and the regression files are printed to the standard
output.

If the code fails the regression test, the coder is responsible for checking why that
happened and do on eof two things: (i) change the code if a bug was introduced, (ii)
update the files in data/regression if code changes introduced legitimate changes to the
output.

"""


import os, sys, codecs, re, getopt, difflib, subprocess
import elsevier1, elsevier2, pubmed, wos, lexisnexis, utils.view
from readers.common import load_data, open_write_file


def process_file(text_file, fact_file, sect_file, collection, language, 
                 fact_type='BAE', verbose=False, html=False):
    """
    Takes a text file and a fact file and creates a .sections file with the section data.
    The data in fact_file can have two formats: (i) the format generated by the BAE
    wrapper and (ii) the format generated by create_standoff.pl. """

    section_factory = create_factory(text_file, fact_file, sect_file, 
                                     fact_type, collection, language, verbose)
    try:
        section_factory.make_sections()
        f = codecs.open(section_factory.sect_file, "w", encoding='utf-8')
        section_factory.print_sections(f)
        f.close()
        if html:
            utils.view.createHTML(text_file, fact_file, fact_file + '.html')
            utils.view.createHTML(text_file, sect_file, sect_file + '.html')
    except UserWarning:
        print 'WARNING:', sys.exc_value


def process_xml_file(xml_file, sect_file, collection, language, verbose=False, html=False):
    """
    Takes a text file and a fact file and creates a .sections file with the section data.
    The data in fact_file can have two formats: (i) the format generated by the BAE
    wrapper and (ii) the format generated by create_standoff.pl. """

    text_xsl = 'utils/standoff/text-content.xsl'
    tags_xsl = 'utils/standoff/standoff.xsl'
    text_file = xml_file + '.txt'
    tags_file = xml_file + '.tags'
    commands = [
        "xsltproc %s %s > %s" % (text_xsl, xml_file, text_file),
        "xsltproc %s %s | xmllint --format - > %s" % (tags_xsl, xml_file, tags_file)]
    pipe = subprocess.PIPE
    for command in commands:
        p = subprocess.Popen(command, shell=True,
                             stdin=pipe, stdout=pipe, stderr=pipe, close_fds=True)
        for line in p.stderr:
            print line

    fact_file =  xml_file + '.fact'
    transform_fact_file(tags_file, fact_file)
    
    process_file(text_file, fact_file, sect_file, collection, language, 
                 fact_type='BASIC', verbose=verbose, html=html)


def transform_fact_file(file1 , file2):
    tags = ('<description', '<abstract', '<technical-field', '<background-art',
            '<claims', '<claim', '<p', '<heading') 
    out = open(file2, 'w')
    for line in open(file1):
        #print line#.strip()#.split()#[0]
        fields = line.strip().split()
        tag = fields[0]
        if tag in tags:
            tagline =  tag[1:] + ' ' + ' '.join(fields[1:])
            tagline = tagline.strip('>')
            out.write(tagline+"\n")

        
def process_files(file_list, collection):
    """
    Takes a file with names of input and output files and processes them. Each line in the
    file has three filenames, separated by tabs, the first file is the text inut file, the
    second the fact input file, and the third the output file."""
    for line in open(file_list):
        (txt_file, fact_file, sections_file) = line.strip().split()
        process_file(txt_file, fact_file, sections_file, collection)

def process_directory(path, collection):
    """
    Processes all files in a directory with text and fact files. Takes all .txt files,
    finds sister files with extension .fact and then creates .sect files."""
    text_files = []
    fact_files= {}
    for f in os.listdir(path):
        if f.endswith('.txt'): text_files.append(f)
        if f.endswith('.fact'): fact_files[f] = True
    total_files = len(text_files)
    file_number = 0
    print "Processing %d files" % total_files
    for text_file in text_files:
        file_number += 1
        fact_file = text_file[:-4] + '.fact'
        sect_file = text_file[:-4] + '.sect'
        if fact_files.has_key(fact_file):
            text_file = os.path.join(path, text_file)
            fact_file = os.path.join(path, fact_file)
            sect_file = os.path.join(path, sect_file)
            print "Processing %d of %d: %s" % (file_number, total_files, text_file[:-4])
            process_file(text_file, fact_file, sect_file, collection)

def create_factory(text_file, fact_file, sect_file,
                   fact_type, collection, language, verbose=False):
    """
    Returns the factory needed given the collection parameter and specifications in the
    fact file and, if needed, some characteristics gathered from the text file."""
    if collection is None:
        collection = determine_collection(fact_file)
    if collection == 'PUBMED': 
        return pubmed.BiomedNxmlSectionFactory(
            text_file, fact_file, sect_file, fact_type, language, verbose)
    elif collection == 'WEB_OF_SCIENCE':
        return wos.WebOfScienceSectionFactory(
            text_file, fact_file, sect_file, fact_type, language, verbose) 
    elif collection == 'LEXISNEXIS':
        return lexisnexis.PatentSectionFactory(
            text_file, fact_file, sect_file, fact_type, language, verbose)
    elif collection == 'ELSEVIER':
        return create_elsevier_factory(
            text_file, fact_file, sect_file, fact_type, language, verbose)

def  create_elsevier_factory(text_file, fact_file, sect_file,
                             fact_type, language, verbose=False):
    """
    Since Elsevier data come in two flavours and each flavour has its own factory, check
    the file to make sure what kind of Elsevier document we are dealing with. It appears
    that counting occurrences of the TEXT type in the fact file predicts whether an
    Elsevier file is structured or not with a precision of about 0.99."""
    fh = open(fact_file)
    text_tags = len( [l for l in fh.readlines() if l.find('TEXT') > -1] )
    fh.close()
    if text_tags < 4 :
        return elsevier1.SimpleElsevierSectionFactory(
            text_file, fact_file, sect_file, fact_type, language)
    else:
        return elsevier2.ComplexElsevierSectionFactory(
            text_file, fact_file, sect_file, fact_type, language, verbose)

def determine_collection(fact_file):
    """
    Loop through the fact file in order to find the line that specifies the collection."""
    expr = re.compile('DOCUMENT.*COLLECTION="(\S+)"')
    for line in open(fact_file):
        result = expr.search(line)
        if result is not None:
            return result.group(1)
    return None

def run_tests():
    """
    Runs a test on four files: a pubmed file, a mockup WOS file, a mockup unstructured
    Elsevier file, and a LexisNexis patent. Prints the output of the document parser as
    well as a diff of that output relative to a file in the data/regression directory."""
    files = (
        'f401516f-bd40-11e0-9557-52c9fc93ebe0-001-gkp847',
        'pubmed-mm-test',
        'elsevier-simple',
        'elsevier-complex',
        'US4192770A',
        'wos'
        )
    results = []
    for f in files:
        text_file = "data/%s.txt" % f
        fact_file = "data/%s.fact" % f
        sect_file = "data/%s.sect" % f
        key_file ="data/regression/%s.sect" % f
        process_file(text_file, fact_file, sect_file, None, None, verbose=False, html=True)
        response = open(sect_file).readlines()
        key = open(key_file).readlines()
        results.append((f, sect_file, response, key_file, key))
    for filename, sect_file, response, key_file, key in results:
        print "\n==> %s (diff)" % filename
        for line in difflib.unified_diff(response, key, fromfile=sect_file, tofile=key_file):
            sys.stdout.write(line)
    print 

def create_ontology_creation_input(xml_file):
    """This code creates input files that can be used by the ontology creation process. it
    currently only works for English."""
    subprocess.call(["perl", "utils/create_standoff.pl", xml_file])    
    text_file = xml_file + '.txt'
    fact_file = xml_file + '.tag'
    sect_file = xml_file + '.sect'
    onto_file = xml_file + '.onto'
    process_file(text_file, fact_file, sect_file, 'LEXISNEXIS', html=True)
    (text, tags) = load_data(text_file, sect_file)
    FIELDS = ('FH_TITLE', 'FH_ABSTRACT', 'FH_SUMMARY', 'FH_DESC_REST', 'FH_DESCRIPTION') 
    FH_DATA = {}
    for f in FIELDS: FH_DATA[f] = None
    for line in open(fact_file):
        (p1, p2, tag) = line.split(' ')[:3]
        if tag == 'invention-title':
            FH_DATA['FH_TITLE'] = (p1, p2, text[int(p1)-1:int(p2)])
            break
    for tag in tags:
        (p1, p2, tagtype) = (tag.start_index, tag.end_index, tag.attr('TYPE'))
        if tagtype == 'ABSTRACT':
            FH_DATA['FH_ABSTRACT'] = (p1, p2, text[int(p1):int(p2)].strip())
        elif tagtype == 'SUMMARY':
            FH_DATA['FH_SUMMARY'] = (p1, p2, text[int(p1):int(p2)].strip())
        elif tagtype == 'DESCRIPTION':
            FH_DATA['FH_DESCRIPTION'] = (p1, p2, text[int(p1):int(p2)].strip())
        desc = FH_DATA['FH_DESCRIPTION']
        summ = FH_DATA['FH_SUMMARY']
        if desc and summ:
            FH_DATA['FH_DESC_REST'] = (summ[1], desc[1], text[summ[1]:desc[1]].strip())
    
    ONTO_FH = open_write_file(onto_file)
    for f in FIELDS[:-1]:
        ONTO_FH.write("%s:\n" % f)
        ONTO_FH.write(FH_DATA[f][2].encode('utf-8'))
        ONTO_FH.write("\n")
    ONTO_FH.write("END\n")


def usage():
    print "\nUsage:"
    print '  % python main.py [-h] [-c COLLECTION] [-l LANGUAGE] TEXT_FILE FACT_FILE STRUCTURE_FILE'
    print '  % python main.py [-h] [-c COLLECTION] [-l LANGUAGE] XML_FILE STRUCTURE_FILE'
    print '  % python main.py [-c COLLECTION] [-l LANGUAGE] FILE_LIST'
    print '  % python main.py [-c COLLECTION] [-l LANGUAGE] DIRECTORY'
    print '  % python main.py -o XML_FILE'
    print '  % python main.py -t'


if __name__ == '__main__':

    try:
        (opts, args) = getopt.getopt(sys.argv[1:], 'htoc:l:')
    except getopt.GetoptError, err:
        print str(err)
        usage()
        sys.exit(2)

    test_mode, html_mode, onto_mode = False, False, False
    collection, language = None, None
    for opt, val in opts:
        if opt == '-t': test_mode = True
        if opt == '-h': html_mode = True
        if opt == '-o': onto_mode = True
        if opt == '-c': collection = val
        if opt == '-l': language = val

    # when called to run some simple tests
    if test_mode:
        run_tests()
            
    elif onto_mode:
        create_ontology_creation_input(args[0])

    # when called to process a text file and a fact file
    elif len(args) == 3:
        text_file, fact_file, sect_file = args[:3]
        collection = args[3] if len(args) > 3 else None
        process_file(text_file, fact_file, sect_file, 
                     collection, language, verbose=False, html=html_mode)

    # when called to process an xml file
    elif len(args) == 2:
        xml_file, sect_file = args[:3]
        process_xml_file(xml_file, sect_file, 
                         collection, language, verbose=False, html=html_mode)

    # processing multiple files
    elif len(args) == 2:
        path = args[1]
        # using directory
        if os.path.isdir(path):
            process_directory(path, collection)
        # using a file that lists files to process
        elif os.path.isfile(path):
            process_files(path, collection)

    # by default
    else:
        text_file = "doc.txt"
        fact_file = "doc.fact"
        sect_file = "doc.sections"
        collection = 'PUBMED'
        process_file(text_file, fact_file, sect_file, 
                     collection, language, verbose=False, html=False)
        
